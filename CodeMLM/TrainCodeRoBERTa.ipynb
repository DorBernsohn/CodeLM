{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-AYaI0aT0hi"
   },
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOKdQdRGW7Ax"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3p0lxsdGT8to"
   },
   "source": [
    "#### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wh9kMeZ4B5Ws"
   },
   "outputs": [],
   "source": [
    "lang = \"Python\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujKzykSsoX7l"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "subprocess.call([\"wget\", f\"https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/{lang}.zip\"])\n",
    "subprocess.call([\"unzip\", f\"/content/{lang}.zip\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApqFy6Holoeq"
   },
   "outputs": [],
   "source": [
    "!mkdir \"log\"\n",
    "log_dir = \"/content/log\"\n",
    "!mkdir \"data\"\n",
    "data_dir = \"/content/data\"\n",
    "!mkdir \"model\"\n",
    "model_dir = \"/content/model\"\n",
    "!mkdir \"tokenizer\"\n",
    "tokenizer_dir = \"/content/tokenizer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGx9muxfRIWV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import (Trainer,\n",
    "                          pipeline,\n",
    "                          RobertaConfig,\n",
    "                          TrainingArguments,\n",
    "                          RobertaForMaskedLM,\n",
    "                          RobertaTokenizerFast,\n",
    "                          LineByLineTextDataset,\n",
    "                          DataCollatorForLanguageModeling)\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNTvCkihOvio"
   },
   "outputs": [],
   "source": [
    "def prepare_text(dir_path):\n",
    "  for path in os.listdir(dir_path):\n",
    "    os.system(f\"gunzip -k {dir_path}/{path}\")\n",
    "\n",
    "  texts = \"\"\n",
    "  for path in os.listdir(dir_path):\n",
    "    if path.endswith(\".jsonl\"):\n",
    "      with open(dir_path + \"/\" + path, 'r') as f:\n",
    "        sample_file = f.readlines()\n",
    "        for sample in sample_file:\n",
    "          obj = json.loads(sample)\n",
    "          texts += obj[\"original_string\"].replace(\"\\n\", \"\").replace(\"\\t\", \"\") + \"\\n\"\n",
    "  return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74QlxA9XP798"
   },
   "outputs": [],
   "source": [
    "train1_texts = prepare_text(f\"/content/{lang}/final/jsonl/train\")\n",
    "train2_texts = prepare_text(f\"/content/{lang}/final/jsonl/valid\")\n",
    "train_texts = train1_texts + \"\\n\" + train2_texts\n",
    "valid_texts = prepare_text(f\"/content/{lang}/final/jsonl/test\")\n",
    "\n",
    "for path, text in zip([\"train_texts.txt\", \"valid_texts.txt\"], \n",
    "                      [train_texts, valid_texts]):\n",
    "  with open(f\"{data_dir}/{path}\",\"w\") as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYCw8stuUAod"
   },
   "source": [
    "#### Train a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mzmSCi_W4vK"
   },
   "outputs": [],
   "source": [
    "paths = [str(x) for x in Path(f\"{data_dir}/\").glob(\"**/*.txt\")]\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "tokenizer.save_model(tokenizer_dir)\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"tokenizer/vocab.json\",\n",
    "    \"tokenizer/merges.txt\",\n",
    ")\n",
    "\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imfRSBrvXEMf"
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LsM9Ofr0XFTU"
   },
   "outputs": [],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ou0DoWzTXGh1"
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_dir, max_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jaJBzKEUFlp"
   },
   "source": [
    "#### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sk6OPYYjXJ7x"
   },
   "outputs": [],
   "source": [
    "model = RobertaForMaskedLM(config=config)\n",
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5exFW-MXMFR"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=f\"{data_dir}/train_texts.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "test_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=f\"{data_dir}/valid_texts.txt\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZwOwm8FXN7a"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cOM_WTXXO_a"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=4,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=5000,\n",
    "    do_eval=True,\n",
    "    logging_dir=log_dir,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset = test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZRNHW93XQZB"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySBTjqqhXSNf"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(tokenizer_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "codeRoBERTa.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
